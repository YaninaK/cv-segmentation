{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaninaK/cv-segmentation/blob/b1/notebooks/03_Models_losses_%26_evalution_score.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKCgu2rHwEc8"
      },
      "source": [
        "# Corrosion detection in steel pipes\n",
        "\n",
        "## 3. Models, Losses, Evaluation score & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUGZlBgnwX7R"
      },
      "source": [
        "* **The objective**:\n",
        "The objective of this challenge is to train a model that have the highest possible score for the segmentation of groove defects using the provided data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initiate = False\n",
        "if initiate:\n",
        "  !git init -q\n",
        "  !git clone -b b1 https://github.com/YaninaK/cv-segmentation.git -q\n",
        "  !pip install -r /content/cv-segmentation/requirements_Colab.txt -q"
      ],
      "metadata": {
        "id": "xPGO2MHIE0cJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "cyyLp9T4hI1z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "NwEz1hVXNNem"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "OKdn6iBISexf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNET Model"
      ],
      "metadata": {
        "id": "ExDUx_8ZSYqM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pAW06SRzxj6"
      },
      "source": [
        "\n",
        "\n",
        "We have chosen a compact architecture for the UNET model üèóÔ∏è consisting of:\n",
        "\n",
        "- 3 down-sampling blocks: (36 x 36), (18 x 18), and (9 x 9).\n",
        "- 3 up-sampling blocks: (9 x 9), (18 x 18), and (36 x 36).\n",
        "\n",
        "Here's an illustration of the UNET model:\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VUS2cCaPB45wcHHFp_fQZQ.png\" alt=\"UNET Model\" width=\"500\" height=\"250\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fnoDG8uNWvPM"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.5):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.enc_conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.enc_conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.enc_conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Additional layers in encoder\n",
        "        self.enc_conv1_additional = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.enc_conv2_additional = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec_conv1 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
        "        self.dec_conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
        "        self.final_conv = nn.Conv2d(32, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = torch.relu(self.enc_conv1(x))\n",
        "        x1 = torch.relu(self.enc_conv1_additional(x1))\n",
        "        x2 = self.maxpool(x1)\n",
        "        x2 = torch.relu(self.enc_conv2(x2))\n",
        "        x2 = torch.relu(self.enc_conv2_additional(x2))\n",
        "        #x2 = self.dropout(x2)\n",
        "        x3 = self.maxpool(x2)\n",
        "        x3 = torch.relu(self.enc_conv3(x3))\n",
        "        x3 = torch.relu(self.enc_conv4(x3))\n",
        "        #x3 = self.dropout(x3)\n",
        "\n",
        "        # Decoder\n",
        "        x = torch.relu(self.upconv1(x3))\n",
        "        x = torch.cat([x2, x], dim=1)\n",
        "        x = torch.relu(self.dec_conv1(x))\n",
        "        x = torch.relu(self.upconv2(x))\n",
        "        x = torch.cat([x1, x], dim=1)\n",
        "        x = torch.relu(self.dec_conv2(x))\n",
        "        x = torch.sigmoid(self.final_conv(x))\n",
        "        #(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –Ω–µ–±–æ–ª—å—à–æ–π ```dropout``` –ª—É—á—à–µ –æ—Å—Ç–∞–≤–∏—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä: ```dropout_prob=0.1```. –ë–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–∏ –ø–æ–¥–±–æ—Ä–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
      ],
      "metadata": {
        "id": "ke12y0vk7jRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-bp9CCjsgfgf",
        "outputId": "5365bbf6-fa43-47e6-c936-cde97920f0e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 36, 36]             320\n",
            "            Conv2d-2           [-1, 32, 36, 36]           9,248\n",
            "         MaxPool2d-3           [-1, 32, 18, 18]               0\n",
            "            Conv2d-4           [-1, 64, 18, 18]          18,496\n",
            "            Conv2d-5           [-1, 64, 18, 18]          36,928\n",
            "         MaxPool2d-6             [-1, 64, 9, 9]               0\n",
            "            Conv2d-7            [-1, 128, 9, 9]          73,856\n",
            "            Conv2d-8            [-1, 128, 9, 9]         147,584\n",
            "   ConvTranspose2d-9           [-1, 64, 18, 18]          32,832\n",
            "           Conv2d-10           [-1, 64, 18, 18]          73,792\n",
            "  ConvTranspose2d-11           [-1, 32, 36, 36]           8,224\n",
            "           Conv2d-12           [-1, 32, 36, 36]          18,464\n",
            "           Conv2d-13            [-1, 1, 36, 36]              33\n",
            "================================================================\n",
            "Total params: 419,777\n",
            "Trainable params: 419,777\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.19\n",
            "Params size (MB): 1.60\n",
            "Estimated Total Size (MB): 3.79\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#Initialize the model\n",
        "model = UNet()\n",
        "model.to(device)\n",
        "summary(model, input_size=(1, 36, 36))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zC-reNrFgaqd",
        "outputId": "499ae707-fcf5-4646-b990-df72e3cea96e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.4831, 0.4830, 0.4826,  ..., 0.4826, 0.4816, 0.4825],\n",
              "          [0.4845, 0.4823, 0.4822,  ..., 0.4797, 0.4810, 0.4817],\n",
              "          [0.4825, 0.4816, 0.4815,  ..., 0.4785, 0.4792, 0.4820],\n",
              "          ...,\n",
              "          [0.4840, 0.4817, 0.4800,  ..., 0.4820, 0.4820, 0.4815],\n",
              "          [0.4831, 0.4824, 0.4812,  ..., 0.4809, 0.4813, 0.4818],\n",
              "          [0.4848, 0.4819, 0.4826,  ..., 0.4837, 0.4818, 0.4823]]]],\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tensor = torch.rand(1,1, 36, 36).to(device)\n",
        "model(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention U-Net"
      ],
      "metadata": {
        "id": "effRUikxXYzt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QiancrguFG"
      },
      "source": [
        "\n",
        "\n",
        "We have chosen to incorporate attention mechanisms into the U-Net to enhance focus on the critical regions of the input image.\n",
        "\n",
        "Here's an illustration of the Attention U-Net architecture:\n",
        "\n",
        "<img src=\"https://idiotdeveloper.com/wp-content/uploads/2021/06/attention_unet-compressed-2.jpg\" alt=\"Attention U-Net architecture\" width=\"500\" height=\"250\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2jPZDjvH6TlX"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Attention block with learnable parameters\"\"\"\n",
        "\n",
        "    def __init__(self, F_g, F_l, n_coefficients):\n",
        "        \"\"\"\n",
        "        :param F_g: number of feature maps (channels) in previous layer\n",
        "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
        "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
        "        \"\"\"\n",
        "        super(AttentionBlock, self).__init__()\n",
        "\n",
        "        self.W_gate = nn.Sequential(\n",
        "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(n_coefficients)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(n_coefficients)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, gate, skip_connection):\n",
        "        \"\"\"\n",
        "        :param gate: gating signal from previous layer\n",
        "        :param skip_connection: activation from corresponding encoder layer\n",
        "        :return: output activations\n",
        "        \"\"\"\n",
        "        g1 = self.W_gate(gate)\n",
        "        x1 = self.W_x(skip_connection)\n",
        "        result = torch.add(g1, x1)\n",
        "        psi = self.relu(result)\n",
        "        #print(g1.shape)\n",
        "        #print(x1.shape)\n",
        "        psi = self.psi(psi)\n",
        "        out = skip_connection * psi\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QQmMSz3lCVWG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AttUNet(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.3):\n",
        "        super(AttUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.enc_conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.enc_conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.enc_conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
        "        self.Att1 = AttentionBlock(F_g=32, F_l=32, n_coefficients=32)\n",
        "        self.dec_conv1 = nn.Conv2d(64, 32, 3, padding=1)\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
        "        self.Att2 = AttentionBlock(F_g=16, F_l=16, n_coefficients=16)\n",
        "        self.dec_conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
        "        self.final_conv = nn.Conv2d(16, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = torch.relu(self.enc_conv1(x)) #32x36x36\n",
        "        e2 = self.maxpool(e1) #32x18x18\n",
        "        e2 = torch.relu(self.enc_conv2(e2)) #64x18x18\n",
        "        e2 = self.dropout(e2) #64x18x18\n",
        "        e3 = self.maxpool(e2) #64x9x9\n",
        "        e3 = torch.relu(self.enc_conv3(e3)) #128x9x9\n",
        "        elif3 = torch.relu(self.enc_conv4(e3)) #128x9x9\n",
        "        e3 = self.dropout(e3)\n",
        "\n",
        "        # Decoder\n",
        "        d2 = torch.relu(self.upconv1(e3)) #[2, 64, 18, 18]\n",
        "        s2 = self.Att1(gate=d2, skip_connection=e2)\n",
        "        d2 = torch.cat([s2, d2], dim=1)\n",
        "        d2 = torch.relu(self.dec_conv1(d2))\n",
        "        #print(d2.shape)\n",
        "        d1 = torch.relu(self.upconv2(d2))\n",
        "        #print(e1.shape)\n",
        "        #print(d1.shape)\n",
        "        s1 = self.Att2(gate=d1, skip_connection=e1)\n",
        "        d1 = torch.cat([s1, d1], dim=1)\n",
        "        #print(d1.shape)\n",
        "        d1 = torch.relu(self.dec_conv2(d1))\n",
        "        out = torch.sigmoid(self.final_conv(d1))\n",
        "        #(x.shape)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResUNet"
      ],
      "metadata": {
        "id": "Uuks23iyXlM2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRcDAYFghwc9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "We have chosen to incorporate  residual connections within the architecture. These residual connections can help to alleviate the vanishing gradient problem and improve the overall performance of the network\n",
        "\n",
        "Here's an illustration of the ResUNet architecture:\n",
        "\n",
        "<img src=\"https://idiotdeveloper.com/wp-content/uploads/2022/01/MultiResUNET.png\" alt=\"ResUNet architecture\" width=\"500\" height=\"250\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CxNYxo2Mav9-"
      },
      "outputs": [],
      "source": [
        "class batchnorm_relu(nn.Module):\n",
        "    def __init__(self, in_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(in_c)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.bn(inputs)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class residual_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\" Convolutional layer \"\"\"\n",
        "        self.b1 = batchnorm_relu(in_c)\n",
        "        self.c1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, stride=stride)\n",
        "        self.b2 = batchnorm_relu(out_c)\n",
        "        self.c2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        \"\"\" Shortcut Connection (Identity Mapping) \"\"\"\n",
        "        self.s = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0, stride=stride)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.b1(inputs)\n",
        "        x = self.c1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.c2(x)\n",
        "        s = self.s(inputs)\n",
        "\n",
        "        skip = x + s\n",
        "        return skip\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "        self.r = residual_block(in_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.upsample(inputs)\n",
        "        #print(x.shape)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.r(x)\n",
        "        return x\n",
        "\n",
        "class build_resunet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\" Encoder 1 \"\"\"\n",
        "        self.c11 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.br1 = batchnorm_relu(64)\n",
        "        self.c12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.c13 = nn.Conv2d(1, 64, kernel_size=1, padding=0)\n",
        "\n",
        "        \"\"\" Encoder 2 and 3 \"\"\"\n",
        "        self.r2 = residual_block(64, 128, stride=2)\n",
        "        #self.r3 = residual_block(128, 256, stride=2)\n",
        "\n",
        "        \"\"\" Bridge \"\"\"\n",
        "        self.r4 = residual_block(128, 256, stride=2)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        #self.d1 = decoder_block(512, 256)\n",
        "        self.d2 = decoder_block(256, 128)\n",
        "        self.d3 = decoder_block(128, 64)\n",
        "\n",
        "        \"\"\" Output \"\"\"\n",
        "        self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Encoder 1 \"\"\"\n",
        "        x = self.c11(inputs)\n",
        "        x = self.br1(x)\n",
        "        x = self.c12(x)\n",
        "        s = self.c13(inputs)\n",
        "        skip1 = x + s\n",
        "\n",
        "        \"\"\" Encoder 2 and 3 \"\"\"\n",
        "        skip2 = self.r2(skip1)\n",
        "        #skip3 = self.r3(skip2)\n",
        "\n",
        "        \"\"\" Bridge \"\"\"\n",
        "        b = self.r4(skip2)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        #d1 = self.d1(b, skip3)\n",
        "        d2 = self.d2(b, skip2)\n",
        "        d3 = self.d3(d2, skip1)\n",
        "\n",
        "        \"\"\" output \"\"\"\n",
        "        output = self.output(d3)\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* –í—Å–µ —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ ```UNET Model```, ```Attention U-Net``` –∏ ```ResUNet``` –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–æ –≤ –±–∞–∑–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –ø–æ–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ç–æ–ª—å–∫–æ ```UNET Model```, –ø–æ—ç—Ç–æ–º—É –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∑–∞–º–µ—Ç–æ–∫ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–∞–±–æ—Ç—ã.\n",
        "* –ï—Å–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª—è—Ö, –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –≤—ã–≤–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ –Ω–æ—É—Ç–±—É–∫, –∞ –≤ –Ω–æ—É—Ç–±—É–∫–µ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n",
        "\n",
        "* –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –º–æ–∂–Ω–æ –ø–æ—ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π:\n",
        "    * UNet++: A Nested U-Net Architecture for Medical Image Segmentation Zongwei Zhou et al., [Jul 2018](https://arxiv.org/abs/1807.10165)\n",
        "    * AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation. Sang et al. [Mar 2022](https://arxiv.org/abs/2105.00402)\n",
        "    * Mask R-CNN. Kaiming He et al. [Jan 2018](https://arxiv.org/abs/1703.06870)\n",
        "    * Vision Transformer (ViT) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Alexey Dosovitskiy et al.[Jun 2021](https://arxiv.org/abs/2010.11929)\n",
        "    * DeiT (data-efficient image transformers)\n",
        "    * VGG16-U-Net\n",
        "  "
      ],
      "metadata": {
        "id": "V_Rcd0SHz2Jh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16MGf14Dih04"
      },
      "source": [
        "## Various Losses for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qaOra2ilt3"
      },
      "source": [
        "### Dice Loss + BCE  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vOJdr61OiohX"
      },
      "outputs": [],
      "source": [
        "class BinaryDiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss of binary class\n",
        "    Args:\n",
        "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
        "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
        "        predict: A tensor of shape [N, *]\n",
        "        target: A tensor of shape same with predict\n",
        "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    Raise:\n",
        "        Exception if unexpected reduction\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "        self.bce=nn.BCELoss()\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
        "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
        "        target = target.contiguous().view(target.shape[0], -1)\n",
        "\n",
        "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
        "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
        "        bce_loss = self.bce(predict, target)\n",
        "        loss = (1 - num / den)+bce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            return loss\n",
        "        else:\n",
        "            raise Exception('Unexpected reduction {}'.format(self.reduction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NT9gRyYi8fW"
      },
      "source": [
        "### Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wIzO136PilCa"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
        "\n",
        "    def __init__(self, gamma=0, size_average=None, ignore_index=-100,\n",
        "                 reduce=None, balance_param=1.0):\n",
        "        super(FocalLoss, self).__init__(size_average)\n",
        "        self.gamma = gamma\n",
        "        self.size_average = size_average\n",
        "        self.ignore_index = ignore_index\n",
        "        self.balance_param = balance_param\n",
        "        self.bce=nn.BCELoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # inputs and targets are assumed to be BatchxClasses\n",
        "        assert len(input.shape) == len(target.shape)\n",
        "        assert input.size(0) == target.size(0)\n",
        "        assert input.size(1) == target.size(1)\n",
        "        # compute the negative likelyhood\n",
        "        bce_loss = self.bce(input.view(-1), target.float().view(-1))\n",
        "        logpt = - bce_loss\n",
        "        pt = torch.exp(logpt)\n",
        "        # compute the loss\n",
        "        focal_loss = -( (1-pt)**self.gamma ) * logpt\n",
        "        balanced_focal_loss = self.balance_param * focal_loss\n",
        "        loss=balanced_focal_loss+bce_loss\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* –í –±–∞–∑–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –º–æ–¥–µ–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å ```Binary Cross Entropy``` - ```torch.nn.BCELoss``` \"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\".\n",
        "\n",
        "* –ï—Å–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å c ```Dice Loss + BCE``` –∏ ```Focal Loss```, —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ª—É—á—à–µ –≤—ã–≤–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ –Ω–æ—É—Ç–±—É–∫, –∞ –≤ –Ω–æ—É—Ç–±—É–∫–µ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."
      ],
      "metadata": {
        "id": "UzXY9MgI26mc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOPFPcBQ8tMf"
      },
      "source": [
        "## Evalution Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1H03DDk133G"
      },
      "source": [
        "### Dice Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ooq9ASbWe78Z"
      },
      "outputs": [],
      "source": [
        "def dice_coeff(prediction, target):\n",
        "\n",
        "    mask = np.zeros_like(prediction)\n",
        "    mask[prediction >= 0.5] = 1\n",
        "\n",
        "    inter = np.sum(mask * target)\n",
        "    union = np.sum(mask) + np.sum(target)\n",
        "    epsilon=1e-6\n",
        "    result = np.mean(2 * inter / (union + epsilon))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–§–æ—Ä–º—É–ª—É –ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–æ–≤ ```pytorch```, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —Å ```pytorch``` –Ω–∞ ```numpy``` –∏ –æ–±—Ä–∞—Ç–Ω–æ:"
      ],
      "metadata": {
        "id": "VfsfyOkQxdUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice(\n",
        "    prediction: torch.tensor, target: torch.tensor, target_one_hot=True\n",
        ") -> torch.tensor:\n",
        "\n",
        "    if not target_one_hot:\n",
        "        target = torch.eye(len(target))[target]\n",
        "\n",
        "    mask = prediction > 0.5\n",
        "\n",
        "    return 2 * (target * mask).sum() / (target.sum() + mask.sum() + 1e-06)"
      ],
      "metadata": {
        "id": "Gw2E5ybxxcgW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = torch.tensor(\n",
        "    [\n",
        "        [0.85, 0.05, 0.05, 0.05],\n",
        "        [0.05, 0.85, 0.05, 0.05],\n",
        "        [0.05, 0.05, 0.85, 0.05],\n",
        "        [0.05, 0.05, 0.05, 0.85]\n",
        "    ]\n",
        ")\n",
        "target = torch.tensor([0, 1, 3, 2])\n",
        "\n",
        "dice(preds, target, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJxX7tNBqVq-",
        "outputId": "935bf148-7be7-4624-be37-2b3c7a173f7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_one_hot = torch.tensor([\n",
        "    [1., 0., 0., 0.],\n",
        "    [0., 1., 0., 0.],\n",
        "    [0., 0., 0., 1.],\n",
        "    [0., 0., 1., 0.]\n",
        "])\n",
        "dice(preds, target_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWCf6wSi0FZl",
        "outputId": "c85f80d5-119e-4409-9695-b25f5d5b81d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ \"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\" - ```torchmetrics.Dice```:"
      ],
      "metadata": {
        "id": "-e1oXr_a1hla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Dice\n",
        "\n",
        "dice_score = Dice()\n",
        "print(dice_score(preds, target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IStyAUFsNYbb",
        "outputId": "301f4d33-df00-47b1-98e7-b74e7eb53ca6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation function"
      ],
      "metadata": {
        "id": "rhQdS1ZT80z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation function\n",
        "def validation(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    total_val_dice_coef = 0\n",
        "    sample = 0\n",
        "    nb_batch = 0\n",
        "    with torch.no_grad():\n",
        "        with tqdm(val_loader, desc='Validation', unit='batch') as tqdm_loader:\n",
        "            for images, masks,_ ,_ in tqdm_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "                masks = masks.float()\n",
        "                images = images.unsqueeze(1)\n",
        "                masks = masks.unsqueeze(1)\n",
        "                pred = model(images)\n",
        "                val_loss = criterion(pred, masks)\n",
        "                y_pred = pred.data.cpu().numpy().ravel()\n",
        "                y_true = masks.data.cpu().numpy().ravel()\n",
        "                val_dice_coef = dice_coeff(y_pred, y_true)\n",
        "                total_val_loss += val_loss.item()\n",
        "                total_val_dice_coef += val_dice_coef.item()\n",
        "                sample += len(images)\n",
        "                nb_batch += 1\n",
        "                tqdm_loader.set_postfix(loss=val_loss.item(), DiceCoef=val_dice_coef.item())\n",
        "    overall_val_loss = total_val_loss / nb_batch\n",
        "    overall_val_dice_coef = total_val_dice_coef / nb_batch\n",
        "    print(f\"Validation Loss: {overall_val_loss}\")\n",
        "    print(f\"Validation Dice Score Coef: {overall_val_dice_coef}\")\n",
        "    return overall_val_loss,overall_val_dice_coef"
      ],
      "metadata": {
        "id": "QsUdgP5f82J3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* –ò–∑-–∑–∞ —Ç–æ–≥–æ —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è ```dice_coeff``` –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å ```torch.tensor```, y_pred –∏ y_true –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –≤ ```numpy```, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ ```cpu```, —á—Ç–æ –∑–∞–º–µ–¥–ª—è–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –§–æ—Ä–º—É–ª–∞ ```dice``` (–≤—ã—à–µ), —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å ```torch.tensor```, –ø–æ–∑–≤–æ–ª–∏—Ç —ç—Ç–æ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å.\n",
        "* –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–∏—Ç—å –∏–∑ –ª–æ–≥–∏–∫–∏, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è - –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è, –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, —è –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è (```training Loop```) –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–π –±–ª–æ–∫ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏."
      ],
      "metadata": {
        "id": "Ghjgprl789wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "j8ybCEfYCPgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = False\n",
        "if run:\n",
        "    #Model\n",
        "    model = UNet()\n",
        "    model.to(device)\n",
        "    #Hyper Parameters\n",
        "    lr = 0.001\n",
        "    weight_decay = 1e-5\n",
        "    betas = (0.9, 0.999)\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    #criterion = FocalLoss() #FocalLoss + BCE\n",
        "    #criterion =BinaryDiceLoss() #DiceLoss + BCE\n",
        "    criterion = nn.BCELoss() #Binary Score Entropy\n",
        "    num_epochs = 25\n",
        "    train_loss=[]\n",
        "    train_dice_score=[]\n",
        "    validation_loss=[]\n",
        "    validation_dice_score=[]\n",
        "    for e in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_dice_coef = 0\n",
        "        sample = 0\n",
        "        nb_batch = 0\n",
        "        with tqdm(train_loader, desc=f'Epoch {e+1}/{num_epochs}', unit='batch') as tqdm_loader:\n",
        "            for images, masks,_,_ in tqdm_loader:\n",
        "                optimizer.zero_grad()\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "                masks = masks.float()\n",
        "                images = images.unsqueeze(1)\n",
        "                masks = masks.unsqueeze(1)\n",
        "                pred = model(images)\n",
        "                loss = criterion(pred, masks).mean().float()\n",
        "                y_pred = pred.data.cpu().numpy().ravel()\n",
        "                y_true = masks.data.cpu().numpy().ravel()\n",
        "                dice_coef = dice_coeff(y_pred, y_true)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "                total_dice_coef += dice_coef.item()\n",
        "                sample += len(images)\n",
        "                nb_batch += 1\n",
        "                tqdm_loader.set_postfix(loss=loss.item(), DiceCoef=dice_coef.item())\n",
        "\n",
        "            overall_loss=total_train_loss/nb_batch\n",
        "            overall_score = total_dice_coef / nb_batch\n",
        "            train_loss.append(overall_loss)\n",
        "            train_dice_score.append(overall_score)\n",
        "        print(f\"Epoch [{e+1}/{num_epochs}], Total Train Loss: {total_train_loss}\")\n",
        "        print(f\"Epoch [{e+1}/{num_epochs}], Dice Score for Training : {overall_score}\")\n",
        "        # Validation\n",
        "        val_loss,val_dice_coef=validation(model, val_loader, criterion, device)\n",
        "        validation_loss.append(val_loss)\n",
        "        validation_dice_score.append(val_dice_coef)\n",
        "        # Save the model\n",
        "        save_dir = \"../results\"\n",
        "        model_path = os.path.join(save_dir, f\"model_epoch_{e+1}.pt\")\n",
        "        torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "BWMjsgH7AFzg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
      ],
      "metadata": {
        "id": "_33vJ74R1aWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. –í—Å–µ —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ ```UNET Model```, ```Attention U-Net``` –∏ ```ResUNet``` –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–æ –≤ –±–∞–∑–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ ```UNET Model```, –ø–æ—ç—Ç–æ–º—É –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∑–∞–º–µ—Ç–æ–∫ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–∞–±–æ—Ç—ã.\n",
        "* –ï—Å–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª—è—Ö, –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –≤—ã–≤–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ –Ω–æ—É—Ç–±—É–∫, –∞ –≤ –Ω–æ—É—Ç–±—É–∫–µ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n",
        "\n",
        "2. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ ```UNET Model```, –Ω–µ–±–æ–ª—å—à–æ–π ```dropout``` –ª—É—á—à–µ –æ—Å—Ç–∞–≤–∏—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä: ```dropout_prob=0.1```. –ë–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–∏ –ø–æ–¥–±–æ—Ä–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
        "\n",
        "3. –í –±–∞–∑–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –º–æ–¥–µ–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å ```Binary Cross Entropy``` (```torch.nn.BCELoss\"```) -  \"\"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\"\".\n",
        "\n",
        "* –ï—Å–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å c ```Dice Loss + BCE``` –∏ ```Focal Loss```, —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ª—É—á—à–µ –≤—ã–≤–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ –Ω–æ—É—Ç–±—É–∫, –∞ –≤ –Ω–æ—É—Ç–±—É–∫–µ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n",
        "\n",
        "4. –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –º–æ–∂–Ω–æ –ø–æ—ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π:\n",
        "    * UNet++: A Nested U-Net Architecture for Medical Image Segmentation Zongwei Zhou et al., [Jul 2018](https://arxiv.org/abs/1807.10165)\n",
        "    * AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation. Sang et al. [Mar 2022](https://arxiv.org/abs/2105.00402)\n",
        "    * Mask R-CNN. Kaiming He et al. [Jan 2018](https://arxiv.org/abs/1703.06870)\n",
        "    * Vision Transformer (ViT) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Alexey Dosovitskiy et al.[Jun 2021](https://arxiv.org/abs/2010.11929)\n",
        "    * DeiT (data-efficient image transformers)\n",
        "    * VGG16-U-Net\n",
        "\n",
        "5. –ò–∑-–∑–∞ —Ç–æ–≥–æ —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è ```dice_coeff``` –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å ```torch.tensor```, ```y_pred``` –∏ ```y_true``` –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –≤ ```numpy``` –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ ```cpu```, —á—Ç–æ –∑–∞–º–µ–¥–ª—è–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –§–æ—Ä–º—É–ª–∞ ```dice``` (–≤—ã—à–µ), —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å ```torch.tensor```, –ø–æ–∑–≤–æ–ª–∏—Ç —ç—Ç–æ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å. –¢–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ \"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\" - ```torchmetrics.Dice```.\n",
        "\n",
        "6. –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–∏—Ç—å –∏–∑ –ª–æ–≥–∏–∫–∏, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è - –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è, –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, —è –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è (```training Loop```) –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–π –±–ª–æ–∫ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏.\n",
        "\n",
        "7. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —è—á–µ–π–∫–µ, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –±–æ–ª—å—à–µ —ç–ø–æ—Ö –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–ø—É—Å–∫—É.\n",
        "\n",
        "8. –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∏ –∑–Ω–∞—á–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –≤—ã–≤–æ–¥–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ ```TensorBoard```, –≤ pytorch –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è ```torch.utils.tensorboard.SummaryWriter```.\n",
        "TensorBoard: –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ TensorFlow - [–∑–¥–µ—Å—å](https://www.tensorflow.org/tensorboard?hl=ru) —Å—Å—ã–ª–∫–∞.\n",
        "\n",
        "  –ö–æ–¥ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏:\n",
        "```\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/corrosion_segmentation_trainer_{}'.format(timestamp))\n",
        "```\n",
        "  –í –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ –ª–æ–≥–∏—Ä—É–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏:\n",
        "```\n",
        "writer.add_scalars(\n",
        "  'Training vs. Validation Loss',\n",
        "  { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "  epoch_number + 1\n",
        ")\n",
        "writer.flush()\n",
        "```\n",
        "  –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ```TensorBoard``` –≤ ```Google Colab```:\n",
        "```\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='/content/cv-segmentation/notebooks/runs'\n",
        "```\n",
        "\n",
        "9. –ò–º–µ–µ—Ç —Å–º—ã—Å–ª –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∏ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –ª—É—á—à–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏:\n",
        "```\n",
        "best_vloss = 1_000_000.\n",
        "if avg_vloss < best_vloss:\n",
        "    best_vloss = avg_vloss\n",
        "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "```\n",
        "10. –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è ```learning rate``` –∫–æ–≥–¥–∞ –º–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–µ—Å—Ç–∞—é—Ç —É–ª—É—á—à–∞—Ç—å—Å—è, –∏–º–µ–µ—Ç —Å–º—ã—Å–ª —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å ```lr_scheduler``` ```ReduceLROnPlateau``` - [–∑–¥–µ—Å—å](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau) —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ \"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\". –£ ```pytorch``` –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã ```lr_scheduler```, –∏—Ö —Ç–æ–∂–µ –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å.\n",
        "\n",
        "11. –ß—Ç–æ–±—ã –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª–æ—Å—å, –µ—Å–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∫–æ–≥–æ-—Ç–æ –Ω–∞–±–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π, –≤ –±–ª–æ–∫–µ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–∞–ª–∏–∑—É–µ–º ```EarlyStopping``` [–∑–¥–µ—Å—å](https://pytorch.org/ignite/generated/ignite.handlers.early_stopping.EarlyStopping.html#ignite.handlers.early_stopping.EarlyStopping) —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ignite. –≠—Ç–æ –±—É–¥–µ—Ç —Å–≤–æ–µ–æ–±—Ä–∞–∑–Ω–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: –∑–∞–¥–∞–µ–º –∑–∞–≤–µ–¥–æ–º–æ –∏–∑–±—ã—Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–æ—Ö, –∏ –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–π –≤—ã–±–æ—Ä–∫–µ –ø–µ—Ä–µ—Å—Ç–∞—é—Ç —É–ª—É—á—à–∞—Ç—å—Å—è –∑–∞–¥–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.\n",
        "\n",
        "12. –ü–æ–≤—ã—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - [–∑–¥–µ—Å—å](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ```pytorch```.\n",
        "\n",
        "13. –ü–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–ª–µ–∑–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –∏ –∏–∑—É—á–∏—Ç—å, –Ω–∞ –∫–∞–∫–∏—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö –º–æ–¥–µ–ª—å –æ—à–∏–±–∞–µ—Ç—Å—è. –≠—Ç–æ –º–æ–∂–µ—Ç –¥–∞—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, –∫–∞–∫ –Ω—É–∂–Ω–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏.\n",
        "\n",
        "14. –§–æ—Ä–º—É–ª—ã, –∑–∞–¥–µ–π—Å—Ç–æ–≤–∞–Ω–Ω—ã–µ –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏, –∏–Ω—Ñ–µ–Ω–µ–Ω—Å–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–∫—Ä—ã—Ç—ã —Ç–µ—Å—Ç–∞–º–∏. –¢–∞–∫ –±—É–¥–µ—Ç —É–¥–æ–±–Ω–µ–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –µ–µ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞. –û–±—ã—á–Ω–æ –¥–ª—è —ç—Ç–∏—Ö —Ü–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ```pytest``` - [–∑–¥–µ—Å—å](https://docs.pytest.org/en/stable/) —Å—Å—ã–ª–∫–∞, ```unittest.mock``` - [–∑–¥–µ—Å—å](https://docs.python.org/3/library/unittest.mock.html) —Å—Å—ã–ª–∫–∞.\n",
        "\n",
        "15. –ï—â–µ –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–∞—Ç—å –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞. –û—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –∏ –∫–∞—á–µ—Å–≤—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è, –∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –ï—Å–ª–∏ –≤ —Ä–∞–∑–º–µ—Ç–∫–µ –æ–±–Ω–∞—Ä—É–∂–∞—Ç—Å—è –∫–∞–∫–∏–µ-—Ç–æ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –Ω–∏–≤–µ–ª–∏—Ä–æ–∞—Ç—å, –µ—Å—Ç—å —à–∞–Ω—Å, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ —É–ª—É—á—à–∞—Ç—Å—è. –î—Ä—É–≥–æ–π –≤–∞—Ä–∏–∞–Ω—Ç - –¥–µ–ª–∞—Ç—å –ø–æ–ø—Ä–∞–≤–∫—É –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ç–∫–∏ - –ø–æ–º–µ—á–∞—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö —Å–ø–æ—Å–æ–±—ã —Ä–∞–∑–º–µ—Ç–∫–∏/ —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤, –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫.\n",
        "\n",
        "16. –ß–∞—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤—ã—à–µ —Ä–µ–∞–ª–∏–∏–∑–æ–≤–∞–Ω–∞ –≤ —Å–∫–≤–æ–∑–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ –≤ –Ω–æ—É—Ç–±—É–∫–µ 04_Baseline_model.ipynb - [c—Å—ã–ª–∫–∞](https://github.com/YaninaK/cv-segmentation/blob/main/notebooks/04_Baseline_model.ipynb) –Ω–∞ –Ω–æ—É—Ç–±—É–∫."
      ],
      "metadata": {
        "id": "lX0ojMdJ1f-7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4451693,
          "sourceId": 7638652,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}